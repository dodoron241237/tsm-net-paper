\documentclass[12pt]{article}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage{extsizes}
\renewcommand{\baselinestretch}{1.5}

\usepackage{fontspec}
\setmainfont{Times New Roman}

\usepackage{xeCJK}
\setCJKmainfont{BiauKai}

\usepackage{fullpage,graphicx,listings,xcolor,booktabs}
\usepackage[hidelinks]{hyperref}
\usepackage[small,bf]{caption}
\setlength{\abovecaptionskip}{5pt plus 3pt minus 2pt}

\definecolor{bgcolor}{rgb}{0.95,0.95,0.95}
\lstdefinestyle{mystyle}{
basicstyle=\footnotesize\ttfamily,
backgroundcolor=\color{bgcolor},
keywordstyle=\color{violet},
stringstyle=\color{red},
commentstyle=\color{gray},
showstringspaces=false,
numbers=left,
frame=line
}
\lstset{
style=mystyle,
breaklines=true,
postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space}
}


\bibliographystyle{acm}

\begin{document}
\begin{titlepage}
\begin{center}
\vspace*{2cm}
{\bf \LARGE TSM-Net: 以對抗式時序壓縮自編碼器為基礎的\\音訊變速演算法} \\
\vspace*{0.3cm}
{\bf \LARGE TSM-Net: Temporal Compressing Autoencoder with Adversarial Losses for Time-Scale Modification on Audio Signals} \\[2cm]
{\large 國立中山大學資訊工程學系} \\
{\large 110 學年度大學部專題製作競賽} \\[2cm]
{\large 組員：B073040018 朱劭璿} \\
{\large \hspace{1.55cm}B072010029 陳居廷} \\[2cm]
{\large 指導教師：陳嘉平 教授}
\end{center}
\end{titlepage}
\newpage
\begin{abstract}
We proposed a novel approach in the field of time-scale modification on the audio signals. While traditional methods use framing technique and spectral approaches use short-time Fourier transform to get high-level units, TSM-Net, our neural-network model encodes the raw audio into a high-level latent representation called neuralgram. Since the resulting neuralgram is a two-dimensional image with real values, we apply some existing image resizing techniques on the neuralgram and decode it using our neural decoder to obtain the time-scaled audio. Our method yields little artifacts and opens a new possibility in the research of modern time-scale modification.
\end{abstract}
\newpage
\tableofcontents
\newpage

\section{Introduction}
With the advance of technologies and digitalization, we can store and reproduce multimedia contents nowadays. We can even manipulate the materials in a way that we couldn't imagine before the digitalization. For example, image resizing and video editing, which changes the dimensionality of the digital pictures spatially and temporally, respectively. Another ubiquitous application regarding audio signals called time-scaled modification (TSM) is used in our daily life. It's also known as playback speed control in the video streaming platform such as YouTube.

With the power of artificial intelligence (AI) and modern computation hardwares, however, we haven't discovered any method using AI to refine TSM algorithm and leverage the quality of the synthetic audio to the next level. Consider we have pragmatic AI tools in similar domains like image super resolution \cite{led17} and motion estimation and motion compensation (MEMC) \cite{bao21}, etc.

\subsection{Time-scale modification}
\paragraph{Time-domain approach}
The main idea of TSM is that instead of scaling the raw waveforms on the time axis, which leads to pitch shifts due to the changes of wavelengths, we frame a sequence of samples, typically larger than the wavelength of the lowest-frequency, and relocate these frames in an overlapping fashion \cite{hej91}\cite{eri90}\cite{ver93}. However, the resultant sound is usually non-natural and contains audible clipping artifacts. This is the negative effect of the framing technique. Moreover, only the most prominent periodicity can be preserved. For the audio with a wide range of frequencies composition, like pop music, symphony and orchestra, the less prominent sound are often erased in the process.

\paragraph{Spectral-domain approach}
Another approach tries to manipulate the audio in the spectral space, using short-time Fourier transform (STFT) to convert the frequency informations from the raw waveform to a more semantic representation with complex numbers \cite{lar99}. The magnitude and phase parts can be further derived. Unfortunately, unlike the magnitudes, which gives constructive and straightforward audio features, the phases is relatively complicated and hard to model. Moreover, due to the heavily correlation between each phase bins, we have to use phase vocoder \cite{fla66} to estimate phases after carefully relocate STFT bins to avoid a peculiar artifacts, a.k.a. phasiness. Despite some refined methods \cite{kra12}\cite{moi11}\cite{nag09}, the spectral representation is essentially not directly scalable, and the iterative phase propagation in the phase vocoder is an inevitable overhead.

\subsection{Harnessing the power of neural networks}
As we've mentioned above, the task requires a highly temporally compressed representation of the audio signals. The neural networks naturally come into our minds. The neural networks are composed of a sequence of linear transformation joined by nonlinear activation functions. With numerous configurable parameters, also known as weights, they are capable of approximating almost any function we desire, including the compression function that transform the raw audio waveforms into low-dimensional latent vectors and back. The parameters are initially random noises and can be gradually inferred using a technique called gradient descent, in which we specify a meaningful loss function such as the difference between the networks' output and the target value, then the parameters can be configured based on the gradients of the loss function so the output would slowly move toward the target value.

\paragraph{TSM-Net}
To transform back and forth in two domains, we can think of the neural networks as an encoder and a decoder, in the jargon of machine learning, this kind of architecture is called autoencoder \cite{kra91}, which encodes the data into a high-level (and typically low-dimensional) latent vectors and tries its best to decode it back to the original data. In our neural network model, the dimension of the latent vectors is 1024 times smaller than the original one, which means one sample in the latent vector can represent more than an entire wave in the raw audio waveform. Since the latent vector is an image-like multi-dimension vector, we can apply the existing image resizing techniques to scale it. Finally, we decode the resized latent vector to obtain the time-scaled audio waveform.

\paragraph{Distribution modeling}
In order to make the model generalizes on the unseen data, we have to model the data distribution instead of directly measuring the L2 distance on the existing dataset, which leads to blurry output upon unseen data. Therefore, we employ a discriminative neural network to help us train the autoencoder \cite{goo14}. The discriminative network computes the adversarial loss, which measures how well it distinguishes the real and generated data. We will go into detail about the training objective and its effectiveness later.

\section{Related Works}
In this section, we summarize the existing methods on TSM, including time-domain ones and spectral-domain ones. We also study the neural vocoder used in text-to-speech (TTS) researches and how it differs from our model. 

\subsection{Traditional TSM methods}
\paragraph{Time-domain TSM}
To prevent the pitch shifting during TSM, most TSM algorithms segment the audio into small chunks of fixed length, a.k.a frames or windows to keep the wavelength intact. In order to minimize the boundary breakage after processing, the adjacent frames are overlapped and rearranged to obtain the synthetic audio. As shown in Figure \ref{fig:1}. The original distance between the start of each frame is called analysis hopsize. After frame relocation, the distance becomes synthesis hopsize, and the ratio of the analysis hopsize and the synthesis hopsize is the rate at which the audio is speed up or down. In addition, the Hann window \cite{ess86} is applied to each analysis frame to maintain the amplitude of overlapped areas. The main challenge is the harmonic alignment problem, as shown in Figure \ref{fig:2}. With significant periodicities, an unconstrained ratio of the analysis to synthesis hopsize can cause a discrepancy with the original waveform. Specifically, the phases of the same frequency components in the frames do not synchronize properly, which leads to serious interference despite the identical waveforms.


\begin{figure}
\begin{center}
  \includegraphics[width=.8\textwidth]{assets/figures/1}
\end{center}
\caption{Generic processing pipeline of time-domain time-scale modification (TSM) procedures.}
\label{fig:1}
\end{figure}

\begin{figure}
\begin{center}
  \includegraphics[width=0.8\textwidth]{assets/figures/2}
\end{center}
\caption{An illustration for harmonic alignment problem. The red Hann windows indicate the rearrangement of the frames. An unconstrained scale ratio would leads to serious interference.}
\label{fig:2}
\end{figure}


\bibliography{tsm-net}

\end{document}
